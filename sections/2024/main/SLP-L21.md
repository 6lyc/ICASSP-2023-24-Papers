# ICASSP-2024-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
    <tr>
        <td><strong>Previous Collections</strong></td>
        <td>
            <a href="https://github.com/DmitryRyumin/ICASSP-2023-24-Papers/blob/main/README_2023.md">
                <img src="http://img.shields.io/badge/ICASSP-2023-0073AE.svg" alt="Conference">
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/ICASSP-2023-24-Papers/blob/main/sections/2024/main/SPTM-P7.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICASSP-2023-24-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICASSP-2023-24-Papers/blob/main/sections/2024/main/SLP-L22.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>


## End-to-End Modeling for Automatic Speech Recognition

![Section Papers](https://img.shields.io/badge/Section%20Papers-6-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-4-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-0-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-0-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Improving Attention-Based End-to-End Speech Recognition by Monotonic Alignment Attention Matrix Reconstruction | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447049-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447049) | :heavy_minus_sign: |
| USM-Lite: Quantization and Sparsity Aware Fine-tuning for Speech Recognition with Universal Speech Models | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10448217-E4A42C.svg)](https://ieeexplore.ieee.org/document/10448217) <br/> [![arXiv](https://img.shields.io/badge/arXiv-2312.08553-b31b1b.svg)](https://arxiv.org/abs/2312.08553) | :heavy_minus_sign: |
| Keep Decoding Parallel with Effective Knowledge Distillation from Language Models to End-to-end Speech Recognisers | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447305-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447305) <br/> [![arXiv](https://img.shields.io/badge/arXiv-2401.11700-b31b1b.svg)](https://arxiv.org/abs/2401.11700) | :heavy_minus_sign: |
| Extreme Encoder Output Frame Rate Reduction: Improving Computational Latencies of Large End-to-End Models | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446985-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446985) <br/> [![arXiv](https://img.shields.io/badge/arXiv-2402.17184-b31b1b.svg)](https://arxiv.org/abs/2402.17184) | :heavy_minus_sign: |
| Improving Multi-Speaker ASR With Overlap-Aware Encoding And Monotonic Attention | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10445900-E4A42C.svg)](https://ieeexplore.ieee.org/document/10445900) | :heavy_minus_sign: |
| On the Relation between Internal Language Model and Sequence Discriminative Training for Neural Transducers | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447468-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447468) <br/> [![arXiv](https://img.shields.io/badge/arXiv-2309.14130-b31b1b.svg)](https://arxiv.org/abs/2309.14130) | :heavy_minus_sign: |